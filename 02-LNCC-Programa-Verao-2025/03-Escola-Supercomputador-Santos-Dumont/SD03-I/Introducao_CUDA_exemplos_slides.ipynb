{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07XvhX6jqXuw"
      },
      "source": [
        "#Introdução à Programação CUDA\n",
        "\n",
        "\n",
        "## **Exemplos**\n",
        "\n",
        "#### Crédito do plugin:\n",
        "Ricardo Ferreira, Michael Canesche, Westerley Carvalho. Universidade Federal de Viçosa - ricardo@ufv.br\n",
        "\n",
        "Minicurso do [Simpósio de Sistemas Computacionais de Alto Desempenho](http://wscad.sbc.org.br/2020/chamada-minicursos.html) (WSCAD 2020). [**Links para video, texto completo e slides**](https://github.com/lesc-ufv/wscad2020/blob/master/links/readme.md)\n",
        "\n",
        "OBS: o minicurso está entre 1h43m25s e 3h38m do vídeo. Se ao clicar você visualizar um erro de reprodução, siga adiante e abra diretamente no YouTube."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhp-cmJnyI1c"
      },
      "source": [
        "## **Configurando o Google Lab (colab)**\n",
        "Executar o comando abaixo para permitir a execução de códigos de maneira mais simplificada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jcVVGLHxwqL"
      },
      "source": [
        "!pip install git+https://github.com/lesc-ufv/cad4u.git &> /dev/null\n",
        "!git clone https://github.com/lesc-ufv/cad4u &> /dev/null\n",
        "%load_ext plugin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0_xTdBP3pbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f93a1661-22ac-45b2-d7d2-fd58d047d1ce"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 29 12:19:48 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execução da aplicação deviceQuery\n",
        "O deviceQuery mostra as principais características do dispositivo instalado, incluindo a CUDA capability."
      ],
      "metadata": {
        "id": "qBkv6-5w1DCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/cuda-samples.git\n",
        "%cd cuda-samples/Samples/1_Utilities/deviceQuery\n",
        "!make\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuiUC-wY0s_z",
        "outputId": "eee36d67-2300-44c8-e103-e134a1735c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cuda-samples'...\n",
            "remote: Enumerating objects: 19507, done.\u001b[K\n",
            "remote: Counting objects: 100% (4370/4370), done.\u001b[K\n",
            "remote: Compressing objects: 100% (752/752), done.\u001b[K\n",
            "remote: Total 19507 (delta 4059), reused 3618 (delta 3618), pack-reused 15137 (from 2)\u001b[K\n",
            "Receiving objects: 100% (19507/19507), 133.52 MiB | 23.92 MiB/s, done.\n",
            "Resolving deltas: 100% (17186/17186), done.\n",
            "Updating files: 100% (4026/4026), done.\n",
            "/content/cuda-samples/Samples/1_Utilities/deviceQuery\n",
            "/usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o deviceQuery.o -c deviceQuery.cpp\n",
            "/usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o deviceQuery deviceQuery.o \n",
            "mkdir -p ../../../bin/x86_64/linux/release\n",
            "cp deviceQuery ../../../bin/x86_64/linux/release\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ./deviceQuery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Fj6HL-P6gbC",
        "outputId": "17d12a6a-ce4a-42a5-909b-0ee8a3bd5f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./deviceQuery Starting...\n",
            "\n",
            " CUDA Device Query (Runtime API) version (CUDART static linking)\n",
            "\n",
            "Detected 1 CUDA Capable device(s)\n",
            "\n",
            "Device 0: \"Tesla T4\"\n",
            "  CUDA Driver Version / Runtime Version          12.2 / 12.2\n",
            "  CUDA Capability Major/Minor version number:    7.5\n",
            "  Total amount of global memory:                 15102 MBytes (15835660288 bytes)\n",
            "  (040) Multiprocessors, (064) CUDA Cores/MP:    2560 CUDA Cores\n",
            "  GPU Max Clock rate:                            1590 MHz (1.59 GHz)\n",
            "  Memory Clock rate:                             5001 Mhz\n",
            "  Memory Bus Width:                              256-bit\n",
            "  L2 Cache Size:                                 4194304 bytes\n",
            "  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)\n",
            "  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n",
            "  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers\n",
            "  Total amount of constant memory:               65536 bytes\n",
            "  Total amount of shared memory per block:       49152 bytes\n",
            "  Total shared memory per multiprocessor:        65536 bytes\n",
            "  Total number of registers available per block: 65536\n",
            "  Warp size:                                     32\n",
            "  Maximum number of threads per multiprocessor:  1024\n",
            "  Maximum number of threads per block:           1024\n",
            "  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n",
            "  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n",
            "  Maximum memory pitch:                          2147483647 bytes\n",
            "  Texture alignment:                             512 bytes\n",
            "  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n",
            "  Run time limit on kernels:                     No\n",
            "  Integrated GPU sharing Host Memory:            No\n",
            "  Support host page-locked memory mapping:       Yes\n",
            "  Alignment requirement for Surfaces:            Yes\n",
            "  Device has ECC support:                        Enabled\n",
            "  Device supports Unified Addressing (UVA):      Yes\n",
            "  Device supports Managed Memory:                Yes\n",
            "  Device supports Compute Preemption:            Yes\n",
            "  Supports Cooperative Kernel Launch:            Yes\n",
            "  Supports MultiDevice Co-op Kernel Launch:      Yes\n",
            "  Device PCI Domain ID / Bus ID / location ID:   0 / 0 / 4\n",
            "  Compute Mode:\n",
            "     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >\n",
            "\n",
            "deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.2, CUDA Runtime Version = 12.2, NumDevs = 1\n",
            "Result = PASS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hello World 0"
      ],
      "metadata": {
        "id": "B0xOArADTuvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%gpu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "\n",
        "__global__ void hello()\n",
        "{\n",
        "\tprintf(\"Oi mundo! De: thread %d - bloco %d\\n\", threadIdx.x, blockIdx.x);\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "\tint num_threads = 2;\n",
        "\tint num_blocks = 2;\n",
        "\thello<<<num_blocks,num_threads>>>();\n",
        "\tcudaDeviceSynchronize();\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "metadata": {
        "id": "4S2vHsLPRvPS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd386312-ab3e-4c87-f14e-56205ff13831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oi mundo! De: thread 0 - bloco 0\n",
            "Oi mundo! De: thread 1 - bloco 0\n",
            "Oi mundo! De: thread 0 - bloco 1\n",
            "Oi mundo! De: thread 1 - bloco 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hello World"
      ],
      "metadata": {
        "id": "C_PfB2wUFxdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%gpu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__device__ const char *STR = \"HELLO WORLD!\";\n",
        "const char STR_LENGTH = 12;\n",
        "\n",
        "__global__ void hello()\n",
        "{\n",
        " \tprintf(\"%c\\n\", STR[threadIdx.x % STR_LENGTH] );\n",
        "\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "\tint num_threads = STR_LENGTH;\n",
        "\tint num_blocks = 1;\n",
        "\thello<<<num_blocks,num_threads>>>();\n",
        "\tcudaDeviceSynchronize();\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "metadata": {
        "id": "hsgjc59gS1UA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e076d693-051f-46e4-a389-1c8641d08daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H\n",
            "E\n",
            "L\n",
            "L\n",
            "O\n",
            " \n",
            "W\n",
            "O\n",
            "R\n",
            "L\n",
            "D\n",
            "!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Add\n",
        "## Versão 1: exemplo de bloco"
      ],
      "metadata": {
        "id": "0S32cmkWYV9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%gpu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 16\n",
        "\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c)\n",
        "{\n",
        "\tint index = threadIdx.x;\n",
        "\tc[index] = a[index] + b[index];\n",
        "\tprintf(\"c[%d] = %d\\n\", index, c[index]);\n",
        "}\n",
        "\n",
        "\n",
        "int main()\n",
        "{\n",
        "  int *a, *b, *c;\n",
        "\tint *d_a, *d_b, *d_c;\n",
        "\tint size = N * sizeof( int );\n",
        "\n",
        "\t/* allocate space for device copies of a, b, c */\n",
        "\n",
        "\tcudaMalloc( (void **) &d_a, size );\n",
        "\tcudaMalloc( (void **) &d_b, size );\n",
        "\tcudaMalloc( (void **) &d_c, size );\n",
        "\n",
        "\t/* allocate space for host copies of a, b, c and setup input values */\n",
        "\n",
        "\ta = (int *)malloc( size );\n",
        "\tb = (int *)malloc( size );\n",
        "\tc = (int *)malloc( size );\n",
        "\n",
        "\tfor( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\ta[i] = b[i] = i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\t/* copy inputs to device */\n",
        "\t/* fix the parameters needed to copy data to the device */\n",
        "\tcudaMemcpy( d_a, a, size, cudaMemcpyHostToDevice );\n",
        "\tcudaMemcpy( d_b, b, size, cudaMemcpyHostToDevice );\n",
        "\n",
        "\t/* launch the kernel on the GPU */\n",
        "\t/* insert the launch parameters to launch the kernel properly using blocks and threads */\n",
        "\tadd<<<1, N >>>( d_a, d_b, d_c );\n",
        "\n",
        "\t/* copy result back to host */\n",
        "\t/* fix the parameters needed to copy data back to the host */\n",
        "\tcudaMemcpy( c, d_c, size, cudaMemcpyDeviceToHost );\n",
        "\n",
        "\t/* clean up */\n",
        "\n",
        "\tfree(a);\n",
        "\tfree(b);\n",
        "\tfree(c);\n",
        "\tcudaFree( d_a );\n",
        "\tcudaFree( d_b );\n",
        "\tcudaFree( d_c );\n",
        "\n",
        "\treturn 0;\n",
        "} /* end main */"
      ],
      "metadata": {
        "id": "nsGexniOXZ8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a8a232-48a8-4dae-fdf7-94132b36748f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c[0] = 0\n",
            "c[1] = 2\n",
            "c[2] = 4\n",
            "c[3] = 6\n",
            "c[4] = 8\n",
            "c[5] = 10\n",
            "c[6] = 12\n",
            "c[7] = 14\n",
            "c[8] = 16\n",
            "c[9] = 18\n",
            "c[10] = 20\n",
            "c[11] = 22\n",
            "c[12] = 24\n",
            "c[13] = 26\n",
            "c[14] = 28\n",
            "c[15] = 30\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Add\n",
        "## Versão 2"
      ],
      "metadata": {
        "id": "iye6Xure6zUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%gpu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c, int n)\n",
        "{\n",
        "    /* insert code to calculate the index properly using blockIdx.x, blockDim.x, threadIdx.x */\n",
        "\tint index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tif(index < n)\n",
        "\t\tc[index] = a[index] + b[index];\n",
        "}\n",
        "\n",
        "/* experiment with N */\n",
        "/* how large can it be? */\n",
        "#define N (2048*2048)\n",
        "#define THREADS_PER_BLOCK 512\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int *a, *b, *c;\n",
        "\tint *d_a, *d_b, *d_c;\n",
        "\tint size = N * sizeof( int );\n",
        "\n",
        "\t/* allocate space for device copies of a, b, c */\n",
        "\n",
        "\tcudaMalloc( (void **) &d_a, size );\n",
        "\tcudaMalloc( (void **) &d_b, size );\n",
        "\tcudaMalloc( (void **) &d_c, size );\n",
        "\n",
        "\t/* allocate space for host copies of a, b, c and setup input values */\n",
        "\n",
        "\ta = (int *)malloc( size );\n",
        "\tb = (int *)malloc( size );\n",
        "\tc = (int *)malloc( size );\n",
        "\n",
        "\tfor( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\ta[i] = b[i] = i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\n",
        "\t/* copy inputs to device */\n",
        "\t/* fix the parameters needed to copy data to the device */\n",
        "\tcudaMemcpy( d_a, a, size, cudaMemcpyHostToDevice );\n",
        "\tcudaMemcpy( d_b, b, size, cudaMemcpyHostToDevice );\n",
        "\n",
        "\t/* launch the kernel on the GPU */\n",
        "\t/* insert the launch parameters to launch the kernel properly using blocks and threads */\n",
        "\tadd<<< (N + (THREADS_PER_BLOCK-1)) / THREADS_PER_BLOCK, THREADS_PER_BLOCK >>>( d_a, d_b, d_c, N);\n",
        "\n",
        "\t/* copy result back to host */\n",
        "\t/* fix the parameters needed to copy data back to the host */\n",
        "\tcudaMemcpy( c, d_c, size, cudaMemcpyDeviceToHost );\n",
        "\n",
        "\n",
        "\tprintf( \"c[0] = %d\\n\",c[0] );\n",
        "\tprintf( \"c[%d] = %d\\n\",N-1, c[N-1] );\n",
        "\n",
        "\t/* clean up */\n",
        "\n",
        "\tfree(a);\n",
        "\tfree(b);\n",
        "\tfree(c);\n",
        "\tcudaFree( d_a );\n",
        "\tcudaFree( d_b );\n",
        "\tcudaFree( d_c );\n",
        "\n",
        "\treturn 0;\n",
        "} /* end main */"
      ],
      "metadata": {
        "id": "qQ5DpbzP5xx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e439382b-7bf8-481d-e499-d6d5af9573ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c[0] = 0\n",
            "c[4194303] = 8388606\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vector Add - Memória unificada"
      ],
      "metadata": {
        "id": "9z8fxUo_YrlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%gpu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void add(int *a, int *b, int *c, int n)\n",
        "{\n",
        "    /* insert code to calculate the index properly using blockIdx.x, blockDim.x, threadIdx.x */\n",
        "\tint index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tif (index < n)\n",
        "\t\tc[index] = a[index] + b[index];\n",
        "}\n",
        "\n",
        "/* experiment with N */\n",
        "/* how large can it be? */\n",
        "#define N (2048*2048)\n",
        "#define THREADS_PER_BLOCK 512\n",
        "\n",
        "int main()\n",
        "{\n",
        "\tint *d_a, *d_b, *d_c;\n",
        "\tint size = N * sizeof( int );\n",
        "\n",
        "\t/* allocate space for device copies of a, b, c */\n",
        "\n",
        "\tcudaMallocManaged(&d_a, size );\n",
        "\tcudaMallocManaged(&d_b, size );\n",
        "\tcudaMallocManaged(&d_c, size );\n",
        "\n",
        "\tfor( int i = 0; i < N; i++ )\n",
        "\t{\n",
        "\t\td_a[i] = d_b[i] = i;\n",
        "\t\td_c[i] = 0;\n",
        "\t}\n",
        "\n",
        "\t/* launch the kernel on the GPU */\n",
        "\t/* insert the launch parameters to launch the kernel properly using blocks and threads */\n",
        "\tadd<<< (N + (THREADS_PER_BLOCK-1)) / THREADS_PER_BLOCK, THREADS_PER_BLOCK >>>( d_a, d_b, d_c, N);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "\tprintf( \"d_c[0] = %d\\n\", d_c[0] );\n",
        "\tprintf( \"d_c[%d] = %d\\n\",N-1, d_c[N-1] );\n",
        "\n",
        "\t/* clean up */\n",
        "\n",
        "\tcudaFree( d_a );\n",
        "\tcudaFree( d_b );\n",
        "\tcudaFree( d_c );\n",
        "\n",
        "\treturn 0;\n",
        "} /* end main */"
      ],
      "metadata": {
        "id": "Sv1FRj0uYmav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fc9021-9d14-4f87-e9b4-d44674eaa619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "d_c[0] = 0\n",
            "d_c[4194303] = 8388606\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1D Stencil**\n"
      ],
      "metadata": {
        "id": "1iiVm2DykYEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%gpu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "#define RADIUS        3\n",
        "#define BLOCK_SIZE    256\n",
        "#define NUM_ELEMENTS  (4096*2)\n",
        "\n",
        "// CUDA API error checking macro\n",
        "#define cudaCheck(error) \\\n",
        "  if (error != cudaSuccess) { \\\n",
        "    printf(\"Fatal error: %s at %s:%d\\n\", \\\n",
        "      cudaGetErrorString(error), \\\n",
        "      __FILE__, __LINE__); \\\n",
        "    exit(1); \\\n",
        "  }\n",
        "\n",
        "__global__ void stencil_1d(int *in, int *out)\n",
        "{\n",
        "    __shared__ int temp[BLOCK_SIZE + 2 * RADIUS];\n",
        "    int gindex = threadIdx.x + (blockIdx.x * blockDim.x) + RADIUS;\n",
        "    int lindex = threadIdx.x + RADIUS;\n",
        "\n",
        "    // Read input elements into shared memory\n",
        "    temp[lindex] = in[gindex];\n",
        "    if (threadIdx.x < RADIUS)\n",
        "    {\n",
        "        temp[lindex - RADIUS] = in[gindex - RADIUS];\n",
        "        temp[lindex + BLOCK_SIZE] = in[gindex + BLOCK_SIZE];\n",
        "    }\n",
        "\n",
        "    // Make sure all threads get to this point before proceeding!\n",
        "    __syncthreads();\n",
        "\n",
        "    // Apply the stencil\n",
        "    int result = 0;\n",
        "    for (int offset = -RADIUS ; offset <= RADIUS ; offset++)\n",
        "        result += temp[lindex + offset];\n",
        "\n",
        "    // Store the result\n",
        "    out[gindex-RADIUS] = result;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  unsigned int i;\n",
        "  int h_in[NUM_ELEMENTS + 2 * RADIUS], h_out[NUM_ELEMENTS];\n",
        "  int *d_in, *d_out;\n",
        "\n",
        "  // Initialize host data\n",
        "  for( i = 0; i < (NUM_ELEMENTS + 2*RADIUS); ++i )\n",
        "    h_in[i] = 1; // With a value of 1 and RADIUS of 3, all output values should be 7\n",
        "\n",
        "  // Allocate space on the device\n",
        "  cudaCheck( cudaMalloc( &d_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int)) );\n",
        "  cudaCheck( cudaMalloc( &d_out, NUM_ELEMENTS * sizeof(int)) );\n",
        "\n",
        "  // Copy input data to device\n",
        "  cudaCheck( cudaMemcpy( d_in, h_in, (NUM_ELEMENTS + 2*RADIUS) * sizeof(int), cudaMemcpyHostToDevice) );\n",
        "\n",
        "  stencil_1d<<< (NUM_ELEMENTS + BLOCK_SIZE - 1)/BLOCK_SIZE, BLOCK_SIZE >>> (d_in, d_out);\n",
        "\n",
        "  cudaCheck( cudaMemcpy( h_out, d_out, NUM_ELEMENTS * sizeof(int), cudaMemcpyDeviceToHost) );\n",
        "\n",
        "  // Verify every out value is 7\n",
        "  for( i = 0; i < NUM_ELEMENTS; ++i )\n",
        "    if (h_out[i] != 7)\n",
        "    {\n",
        "      printf(\"Element h_out[%d] == %d != 7\\n\", i, h_out[i]);\n",
        "      break;\n",
        "    }\n",
        "\n",
        "  if (i == NUM_ELEMENTS)\n",
        "    printf(\"SUCCESS!\\n\");\n",
        "\n",
        "  // Free out memory\n",
        "  cudaFree(d_in);\n",
        "  cudaFree(d_out);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "kXF3RUUvkdX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stencil 2D com memória compartilhada\n",
        "\n",
        "Este exemplo usa um array 2D de entrada preenchido com números predefinidos ou gerados aleatoriamente. Ele aplica o stencil 2D para calcular a média da vizinhança (3x3) e imprime os valores de entrada e saída."
      ],
      "metadata": {
        "id": "QONevAls4k10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%gpu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define RADIUS 1\n",
        "#define BLOCK_SIZE 8\n",
        "\n",
        "// Kernel CUDA para aplicar stencil 2D\n",
        "__global__ void stencil2D_shared(const float *input, float *output, int width, int height) {\n",
        "    __shared__ float sharedMem[BLOCK_SIZE + 2 * RADIUS][BLOCK_SIZE + 2 * RADIUS];\n",
        "\n",
        "    // Coordenadas globais\n",
        "    int globalX = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n",
        "    int globalY = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n",
        "\n",
        "    // Coordenadas locais\n",
        "    int localX = threadIdx.x + RADIUS;\n",
        "    int localY = threadIdx.y + RADIUS;\n",
        "\n",
        "    // Carregar os dados principais para a memória compartilhada\n",
        "    if (globalX < width && globalY < height) {\n",
        "        sharedMem[localY][localX] = input[globalY * width + globalX];\n",
        "    } else {\n",
        "        sharedMem[localY][localX] = 0.0f;\n",
        "    }\n",
        "\n",
        "    // Carregar as bordas\n",
        "    if (threadIdx.x < RADIUS) {\n",
        "        sharedMem[localY][threadIdx.x] = (globalX >= RADIUS) ? input[globalY * width + (globalX - RADIUS)] : 0.0f;\n",
        "        sharedMem[localY][threadIdx.x + BLOCK_SIZE + RADIUS] =\n",
        "            (globalX + BLOCK_SIZE < width) ? input[globalY * width + (globalX + BLOCK_SIZE)] : 0.0f;\n",
        "    }\n",
        "\n",
        "    if (threadIdx.y < RADIUS) {\n",
        "        sharedMem[threadIdx.y][localX] = (globalY >= RADIUS) ? input[(globalY - RADIUS) * width + globalX] : 0.0f;\n",
        "        sharedMem[threadIdx.y + BLOCK_SIZE + RADIUS][localX] =\n",
        "            (globalY + BLOCK_SIZE < height) ? input[(globalY + BLOCK_SIZE) * width + globalX] : 0.0f;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Aplicar stencil\n",
        "    if (globalX < width && globalY < height) {\n",
        "        float sum = 0.0f;\n",
        "        for (int dy = -RADIUS; dy <= RADIUS; dy++) {\n",
        "            for (int dx = -RADIUS; dx <= RADIUS; dx++) {\n",
        "                sum += sharedMem[localY + dy][localX + dx];\n",
        "            }\n",
        "        }\n",
        "        output[globalY * width + globalX] = sum / ((2 * RADIUS + 1) * (2 * RADIUS + 1));\n",
        "    }\n",
        "}\n",
        "\n",
        "void printMatrix(const char *label, float *matrix, int width, int height) {\n",
        "    printf(\"%s:\\n\", label);\n",
        "    for (int i = 0; i < height; i++) {\n",
        "        for (int j = 0; j < width; j++) {\n",
        "            printf(\"%6.2f \", matrix[i * width + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Dimensões da matriz\n",
        "    int width = 16;\n",
        "    int height = 16;\n",
        "\n",
        "    // Tamanho em bytes\n",
        "    size_t size = width * height * sizeof(float);\n",
        "\n",
        "    // Alocar e inicializar a matriz no host\n",
        "    float *h_input = (float *)malloc(size);\n",
        "    float *h_output = (float *)malloc(size);\n",
        "\n",
        "    for (int i = 0; i < width * height; i++) {\n",
        "        h_input[i] = rand() % 10 + 1; // Valores aleatórios entre 1 e 10\n",
        "    }\n",
        "\n",
        "    // Alocar memória no dispositivo\n",
        "    float *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, size);\n",
        "    cudaMalloc(&d_output, size);\n",
        "\n",
        "    // Copiar a matriz de entrada para o dispositivo\n",
        "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Configurar dimensões de bloco e grid\n",
        "    dim3 blockSize(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 gridSize((width + BLOCK_SIZE - 1) / BLOCK_SIZE, (height + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "    // Chamar o kernel\n",
        "    stencil2D_shared<<<gridSize, blockSize>>>(d_input, d_output, width, height);\n",
        "\n",
        "    // Copiar o resultado de volta para o host\n",
        "    cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Imprimir matrizes de entrada e saída\n",
        "    printMatrix(\"Matriz de entrada\", h_input, width, height);\n",
        "    printMatrix(\"Matriz de saída (após stencil)\", h_output, width, height);\n",
        "\n",
        "    // Liberar memória\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "Lxmpgrp92GfW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}